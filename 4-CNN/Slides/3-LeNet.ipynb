{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb042ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# LeNet\n",
    "\n",
    "Among the first ground breaking result for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db98b7a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470524da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the 90s, optical character recognition was a hot topic  \n",
    "Especially for banks who had to had to process millions of bank check a year (at that time, credit cards were not as popular as today)\n",
    "\n",
    "In 1998, Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner proposed a CNN architecture and a gradient-based approach to train it on handwritten character recognition (**MNIST** dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b23dca5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube-nocookie.com/embed/FwFduRA_L6Q\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1488cb670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.youtube-nocookie.com/embed/FwFduRA_L6Q',width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdec2d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src='images/mnist.png' width=\"65%\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <p style=\"font-size:14px;\">Source: <a href='http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf'>LeCun's MNIST paper</a></p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74d68f2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class LeCunRevisited(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeCunRevisited, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2) # original uses AvgPool, kernel size 2 divide the ouput by 4\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv1(x)))\n",
    "        x = self.max_pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x)) # original uses Sigmoid\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e019d6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1359,  0.0311, -0.0336, -0.0388,  0.0132,  0.0497,  0.0007, -0.0014,\n",
       "         -0.1168,  0.0448],\n",
       "        [-0.1307,  0.0708, -0.0664, -0.0187, -0.0031,  0.0550,  0.0124,  0.0043,\n",
       "         -0.0965,  0.0405]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeCunRevisited()\n",
    "net(torch.randn(2, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc48ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's now solve MNIST using *LeCun* architecture\n",
    "\n",
    "First, let's download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d75d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "mnist_train = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b9f34e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of test examples: 10000\n",
      "Image size: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(mnist_train)}')\n",
    "print(f'Number of test examples: {len(mnist_test)}')\n",
    "print(f'Image size: {mnist_train[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb5f61",
   "metadata": {},
   "source": [
    "Notice that the MNIST data from *torchvision* are 28x28 compare to the 32x32 from *LeCun*'s paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562c138",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's always a good idea to look at our data before doing anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69b0e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show(img, title=None):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "show(mnist_train[0][0], mnist_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb50ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's create the dataloaders so we can have batch to train/test our network on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4b135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataloader_train = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_dataloader_test = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33422f77",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have to define a *LeCun* model that is compatible with the 28*28 input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a102fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeCunRevisited(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (adaptive_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeCunRevisited(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeCunRevisited, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        self.fc = nn.Linear(32, 10)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool(F.relu(self.conv1(x)))\n",
    "        x = self.max_pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.adaptive_avg_pool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = LeCunRevisited()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18422bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our traditional training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a945e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0f10715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2hefks79) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2577... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1.71MB of 1.71MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">winter-galaxy-7</strong>: <a href=\"https://wandb.ai/ingambe/LeCunExample/runs/2hefks79\" target=\"_blank\">https://wandb.ai/ingambe/LeCunExample/runs/2hefks79</a><br/>\n",
       "Find logs at: <code>./wandb/run-20221128_115051-2hefks79/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2hefks79). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/ingambe/LeCunExample/runs/1c41vhlb\" target=\"_blank\">apricot-firebrand-8</a></strong> to <a href=\"https://wandb.ai/ingambe/LeCunExample\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='LeCunExample')\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    i = 0\n",
    "    for i, data in enumerate(mnist_dataloader_train):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            running_loss += loss.cpu().item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = predicted.cpu()\n",
    "            running_acc += (predicted == labels).float().mean().item()\n",
    "    wandb.log({'loss': running_loss / (i + 1),\n",
    "                'acc': running_acc / (i + 1),\n",
    "                'epoch': epoch + 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cc213",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now need to check the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in mnist_dataloader_test:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the network on the {total} test images dataset: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a4cbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great! Now let's save our awesome model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist.pth')\n",
    "wandb.save('mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7f645",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
